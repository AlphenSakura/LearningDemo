{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__(self, vocab_size, d_model, padding_idx=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super().__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model, device)\n",
    "        self.encoding.requires_grad_(False)\n",
    "        \n",
    "        pos = torch.arange(0, max_len, device)\n",
    "        pos = pos.float().unsqueeze(1)\n",
    "        _2i = torch.arange(0, d_model, device)\n",
    "        \n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (1000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (1000 ** (_2i / d_model)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape(0)\n",
    "        return self.encoding[:seq_len, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-10):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.std(-1, unbiased=False, keepdim=True)\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return out * self.gamma +self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention (nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.w_combine = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch, time, dimension = q.shape\n",
    "        n_d = self.d_model // self.num_heads\n",
    "        \n",
    "        # perform linear operation and split into N heads\n",
    "        q, k, v = self.q_linear(q), self.k_linear(k), self.v_linear(v)\n",
    "\n",
    "        q = q.view(batch, time, self.num_heads, n_d).permute(0, 2, 1, 3)\n",
    "        k = k.view(batch, time, self.num_heads, n_d).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch, time, self.num_heads, n_d).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # score = torch.matmul(q, k.permute(0, 1, 3, 2)) / (self.d_model ** 0.5)\n",
    "        score = q @ k.transpose(2, 3) / math.sqrt(n_d)\n",
    "        mask = torch.tril(torch.ones(time, time, dtype=bool))\n",
    "        score = score.masked_fill(mask == 0 , float('-inf'))\n",
    "        score = self.softmax(score) @ v\n",
    "        \n",
    "        score = score.permute(0, 2, 1, 3).contiguous().view(batch, time, dimension)\n",
    "        \n",
    "        output = self.w_combine(score)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2621, -0.0387,  0.0226,  ..., -0.0882,  0.1103,  0.2545],\n",
      "         [-0.1952, -0.1403,  0.0070,  ..., -0.0733,  0.1259,  0.3139],\n",
      "         [-0.2382, -0.0973,  0.0177,  ..., -0.0724,  0.0650,  0.3530],\n",
      "         ...,\n",
      "         [-0.1946, -0.0888,  0.0662,  ..., -0.0739,  0.0375,  0.3863],\n",
      "         [-0.1958, -0.0904,  0.0670,  ..., -0.0757,  0.0388,  0.3863],\n",
      "         [-0.1953, -0.0912,  0.0685,  ..., -0.0747,  0.0381,  0.3840]],\n",
      "\n",
      "        [[-0.1965, -0.1198,  0.0130,  ..., -0.1202,  0.0647,  0.5903],\n",
      "         [-0.2062, -0.1379,  0.1116,  ..., -0.1313,  0.0786,  0.4496],\n",
      "         [-0.2256, -0.1305,  0.0948,  ..., -0.1183,  0.1172,  0.4691],\n",
      "         ...,\n",
      "         [-0.2050, -0.0943,  0.0607,  ..., -0.0677,  0.0321,  0.4063],\n",
      "         [-0.2061, -0.0949,  0.0614,  ..., -0.0655,  0.0304,  0.4094],\n",
      "         [-0.2026, -0.0953,  0.0622,  ..., -0.0655,  0.0307,  0.4061]],\n",
      "\n",
      "        [[-0.3103, -0.0166,  0.1777,  ..., -0.1316, -0.0570,  0.4131],\n",
      "         [-0.3553, -0.1048,  0.0973,  ..., -0.0805,  0.0504,  0.4010],\n",
      "         [-0.2671, -0.0474,  0.1177,  ..., -0.0611,  0.0461,  0.3760],\n",
      "         ...,\n",
      "         [-0.2119, -0.0998,  0.0689,  ..., -0.0529,  0.0471,  0.3779],\n",
      "         [-0.2102, -0.0978,  0.0701,  ..., -0.0544,  0.0466,  0.3776],\n",
      "         [-0.2110, -0.0967,  0.0699,  ..., -0.0516,  0.0462,  0.3769]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0775, -0.1320,  0.1034,  ...,  0.0214, -0.0316,  0.3890],\n",
      "         [-0.1038, -0.0698,  0.0798,  ...,  0.1264,  0.0212,  0.2721],\n",
      "         [-0.0910, -0.0704,  0.0434,  ...,  0.0472,  0.0735,  0.3018],\n",
      "         ...,\n",
      "         [-0.1890, -0.0772,  0.0712,  ..., -0.0733,  0.0374,  0.3774],\n",
      "         [-0.1895, -0.0766,  0.0724,  ..., -0.0700,  0.0363,  0.3804],\n",
      "         [-0.1908, -0.0768,  0.0704,  ..., -0.0729,  0.0345,  0.3802]],\n",
      "\n",
      "        [[-0.2255, -0.1390,  0.0359,  ...,  0.0281,  0.1750,  0.2432],\n",
      "         [-0.2487, -0.0303,  0.0602,  ...,  0.0231,  0.0748,  0.2095],\n",
      "         [-0.2658, -0.0921,  0.0029,  ...,  0.0604,  0.0696,  0.2735],\n",
      "         ...,\n",
      "         [-0.2293, -0.0802,  0.0629,  ..., -0.0679,  0.0508,  0.4102],\n",
      "         [-0.2301, -0.0792,  0.0617,  ..., -0.0667,  0.0501,  0.4071],\n",
      "         [-0.2290, -0.0814,  0.0609,  ..., -0.0652,  0.0532,  0.4068]],\n",
      "\n",
      "        [[-0.1799, -0.0246,  0.0234,  ..., -0.1927,  0.1988,  0.3616],\n",
      "         [-0.2886, -0.0349,  0.0517,  ..., -0.0806,  0.1476,  0.2835],\n",
      "         [-0.2645, -0.0081,  0.0414,  ..., -0.0341,  0.1153,  0.2985],\n",
      "         ...,\n",
      "         [-0.1911, -0.0847,  0.0756,  ..., -0.0517,  0.0193,  0.3989],\n",
      "         [-0.1895, -0.0816,  0.0744,  ..., -0.0531,  0.0185,  0.3995],\n",
      "         [-0.1884, -0.0807,  0.0735,  ..., -0.0527,  0.0204,  0.3981]]],\n",
      "       grad_fn=<AddBackward0>) torch.Size([128, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "n_heads = 8\n",
    "X = torch.rand(128, 64, d_model)\n",
    "attention = multi_head_attention(d_model, n_heads)\n",
    "out = attention(X, X, X)\n",
    "print(out, out.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
